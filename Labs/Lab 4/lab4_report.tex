\documentclass[a4paper]{article}

\usepackage[]{babel}
\usepackage[latin1]{inputenc}
\usepackage{amssymb}
\usepackage{framed}
\setlength{\parindent}{0pt}
\setlength{\parskip}{3ex}
\usepackage{biblatex}


\bibliography{Labs/Lab 4/ref}

\begin{document}

\begin{center}
  {\large Artificial Neural Networks and Deep Architectures, DD2437}\\
  \vspace{7mm}
  {\huge Short report on lab assignment 4\\[1ex]}
  {\Large Restricted Boltzmann Machines and
Deep Belief Nets}\\
  \vspace{8mm}  
  {\Large Rakin Ali, Steinar Logi and Hassan Alzubeidi\\}
  \vspace{4mm}
  {\large March 4, 2024\\}
\end{center}



\section{Main objectives and scope of the assignment}

The main objective with this lab is to familiarize ourselves with Deep Belief Nets (DBN). Deep belief nets are a Deep Neural Network architecture combined of multiple Restricted Boltzmann Machines that are pre-trained using unsupervised greedy pre-training, using Contrastive Divergence. The main objectives are the following:  
\begin{itemize}
\item To Familiarize ourselves with the key ideas underlying the learning process of Restricted Boltzmann Machines.
\item Design a Deep Belief Network and train it using greedy pre-training.
\item Study the Generative aspects of DBN
\end{itemize}
Deep Neural Network architectures are a class of many different networks that can exploit the expressive powers of deep architectures, for example using Autoencoders to pre-train the hidden layers. In this lab we will, however, focus on a certain subset of deep architectures, namely Deep Belief Nets, which use Restricted Boltzmann Machines.

\section{Methods}
A code skeleton was generated for us to use however all of us modified it to be able to complete this lab. All of us did this separately which gave us better insights on how to build the models however only one of us could complete the lab code-wise fully. 

\section{Results and discussion}
Section \ref{part1} covers task 4.1 where an RBM was used for recognizing MNIST images. Section \ref{part2} covers greedy layer-wise pertaining which corresponds to section 4.2. Most of the ideas were taken from the two reports by Hinton \cite{hinton2006fast} \cite{hinton2012practical}. The one which came out recently was more useful for this lab. 

\subsection{Restricted Bolzmann Machine}
\label{part1}
Restricted Boltzmann Machines (RBMs) have hidden nodes and visible nodes. The activations of the nodes are probabilistic and not deterministic. Each node is a sigmoid activation function and the input is the weighted sum of all the other nodes. The activation is the probability of node being turned on or not can be seen below:

$$
p(v_i = 1) = \sigma (\sum_i w_{ij}v_{i})
$$
The Restricted Boltzmann machine is a subset of Boltzmann Machines where connections within layers are omitted. The Restricted Boltzmann Machine can therefore be represented as bipartide graph. When there are no connections within the layer, the visible nodes are conditionally independent given the hidden nodes and vice versa, that is:
$$
p(\textbf{v} |  \textbf{h}) = \Pi_{i}p(v_i | \textbf{h})
$$
The Restricted Boltzmann Machine is trained using Contrastive Divergence. To use Contrastive Divergence we need to perform alternating Gibbs sampling on the network to obtain the updates of the weights. The update between the ith visible unit and jth hidden unit is: $\Delta w_{ij} \propto <v_i h_j>^{t=0} - <v_i h_j>^{t=k}$. What Contrastive Divergence essentially does is that it maximizes the likelihood of the joint probability $p(\textbf{v}, \textbf{h})$. That is the visible states in the dataset are assigned maximum probability. The hidden vectors therefore describe some latent features of the data set that we assume are useful. This is then exploited in Deep Belief Networks by pre-training separate Restricted Boltzmann Machines. We will see this in part 2 of this lab.

Monitoring stability was an interesting task, Allowing to the literature, \textit{"the construction error is actually a very poor measure of the progress of learning"} \cite{hinton2012practical}. In short terms use it but don't trust it. If you want to know then use graphic displays. This can be seen below. 
\begin{figure}
    \centering
    \includegraphics{Labs/lab 4}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}

We created a Restricted Boltzmann Machine and trained it using Contrastive Divergence. We used the MNIST dataset with handwritten digits. Each image in this data set is 28 by 28 pixels, that is each input vector when flattened has 784 dimensions. We split the dataset into a training set and test set. We used a Restricted Boltzmann Machine with 500 hidden units and trained it using Contrastive Divergence on the training set. We used a batch size of 20 and trained the Restricted Boltzmann Machine for 20 epochs. By monitoring the reconstruction error we theorized that we could monitor if the RBM achieved some kind of stability. The reconstruction error was 

\subsection{Greedy-later-wise Pretraining}
\label{part2}


\section{Final remarks \normalsize{\textit{(max 0.5 page)}}}
\textit{Please share your final reflections on the lab, its content and your own learning. Which parts of the lab assignment did you find confusing or not necessarily helping in understanding important concepts and which parts you have found interesting and relevant to your learning experience? \\
Here you can also formulate your opinion, interpretation or speculation about some of the simulation outcomes. Please add any follow-up questions that you might have regarding the lab tasks and the results you have produced.}

\printbibliography


\end{document}
