\documentclass[a4paper]{article}

\usepackage[swedish]{babel}
\usepackage[latin1]{inputenc}
\usepackage{amssymb}
\usepackage{framed}

\setlength{\parindent}{0pt}
\setlength{\parskip}{3ex}

\begin{document}

\begin{center}
  {\large Artificial Neural Networks and Deep Architectures, DD2437}\\
  \vspace{7mm}
  {\huge Short report on lab assignment 1b\\[1ex]}
  {\Large Learning with backpropagation and generalisation in multi-layer perceptrons\\}
  \vspace{8mm}  
  {\Large Rakin Ali , Steinar Logi and Hasan \textbf{EFTERNAMN}\\}
  \vspace{4mm}
  {\large January 31 2024 }
\end{center}

\section{Main objectives and scope of the assignment}
The major objectives in this assignment was
\begin{itemize}
\item To gain a better theoretical foundation of Neural Networks learn by implementing Backpropagation algorithm and monitoring its parameters. 
\item Building Multilayered Perceptrons from scratch with plain Python programming using no imports and by using libraries used by the industry, in this case it was TensorFlow.
\item to observe how to improve generalization of a model and which parameters in the model has the most effects on generalization. 
\end{itemize}
The scope of this assignment are the instructions that were given excluding all non-mandatory tasks. The limitations was that all data were supervised and only one hidden layer was created. We did as the instructions and did not try different activation functions or making the MLP deeper. 
\section{Methods}
Most of the lab instructions were conducted twice in order to check if the results were reasonable but also so that each of us had gain a better theoretical foundation of the assignments. We discussed and had to ask ourselves \textit{is this result reasonable or is there a mistake in the code} and several times there were a few mistakes which were fixed. The amount of hidden nodes was modified together with the learning rate. What was discovered was that as long as there were more than one hidden node, the model achieved good results however more nodes resulted in a more complex boundary. 
\begin{figure}
    \centering
    \includegraphics{}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}


\section{Results and discussion}
\subsection{Classification and regression with a two-layer perceptron \textit{(ca.2 pages)}}
\textbf{Classification of linearly non-separable data:} For this assignment linearly non-separable data was generated. In the first part of the number of hidden nodes in the hidden layer was modified in order to study the performan

%\subsubsection{The encoder problem}
%\textit{Here you do not really need any illustrations, this could be a very short section reporting on your experiments in line with the assignment questions.}

\subsubsection{Function approximation}
\textit{This subsection requires plots to reflect intuitive visual interpretation of the results. Make sure that you condense information and avoid any excessive plotting. Here you might also need to incorporate some illustration of the network's generalisation performance or use a table to systematically report the results requested in the assignment.}

\subsection{Multi-layer perceptron for time series prediction \textit{(ca.3 pages)}}

\textit{Here you do not have to introduce the problem or define Mackey-Glass time series, as you should focus on the results. You could divide them into two parts as the following two suggested subsections but you might as well keep your story under the main heading of this part of the assignment. Importantly, always clearly state what network architecture you use, crucially with the number of hidden nodes, systematically report average results with various manipulations (regularisation etc.) and pay attention to differences between training, validation and test errors. Illustrating the outcome of your network predictions along with the original chaotic time series can also be very helpful. Finally, since you compare two- and three-layer architectures, make sure that you do not jump to any conclusions based on a small number of simulations unless you have statistically convincing evidence (when you comare the mean performance measures, their second moment is also relevant). In this part it may be particularly desirable to rely on tables.}

\subsubsection{Three-layer perceptron for time series prediction - model selection, validation}

\subsubsection{Three-layer perceptron for noisy time series prediction with penalty regularisation}

\section{Final remarks \normalsize{\textit{(max 0.5 page)}}}
\textit{Please share your final reflections on the lab, its content and your own learning. Which parts of the lab assignment did you find confusing or not necessarily helping in understanding important concepts and which parts you have found interesting and relevant to your learning experience? \\
Here you can also formulate your opinion, interpretation or speculation about some of the simulation outcomes. Please add any follow-up questions that you might have regarding the lab tasks and the results you have produced.}

\end{document}
